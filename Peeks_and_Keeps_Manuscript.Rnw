
%\documentclass{nature,floatsintext}


\documentclass[a4paper,doc,natbib,floatsintext]{apa6}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{float}

\title{Peeks and Keeps: A new paradigm for studying the exploration-exploitation trade-off}
\shorttitle{Peeks and Keeps}
\twoauthors{Nathaniel D. Phillips and Hans Joerg Neth}{Daniel Navarro}
\twoaffiliations{University of Konstanz}{University of Adelaide}

\abstract{Many important decision tasks involve an exploration-exploitation trade-off, where organisms have the competing goals of gaining new information (exploration) to improve future decisions, and acting on existing information (exploitation). The most common paradigm to study this trade-off experimentally is the n-armed bandit, where decision makers reap real costs and rewards on every trial. We suggest that, unlike the n-armed bandit, many real world tasks allow decision makers to explore options (such as stock price changes) without reaping any costs or rewards. To address this, we introduce a new experimental paradigm called ``Peeks and Keeps'' that combines aspects of the n-armed bandit with the `bet-observe' task \citep{tversky1966information}. Unlike the n-armed bandit, Peeks and Keeps gives decision makers the option of explicitly separating exploration and exploitation behavior, where exploration provides only information but no costs or rewards, and exploitation gives both information and costs and rewards. This paradigm not only increases the empirical validity of the n-armed bandit, but also provides researchers with an explicit measure of exploration that is hidden in other paradigms.}

\keywords{exploration, exploitation, decisions from experience, decisions under uncertainty}


\begin{document}

<<echo = F, message = F>>=
library("xtable")
library("dplyr")
library("snowfall")
library("RColorBrewer")
library("yarrr")
library("beanplot")
library("MCMCglmm")
library("RColorBrewer")
library("BEST")
library("xtable")
library("stringr")
library("R2jags")
library("BayesFactor")
options(stringsAsFactors = F)
@



% Custom R Functions
<<echo = F, message = F>>=

hdi.text.fun <- function(x, 
                         numSavedSteps = 5000, 
                         burnInSteps = 100, 
                         incl.mean = T, 
                         digits = 2,
                         calculate = T, wordy = F
                         ) {
  
if(calculate == T) {
  
mcmc <- BESTmcmc(x, numSavedSteps = numSavedSteps, thinSteps = 1, burnInSteps = burnInSteps, verbose = F)
hdi.t <- hdi(mcmc$mu)
}

if(calculate == F) {hdi.t <- hdi(x)}

if(incl.mean == F) {

hdi.text <- paste("[", round(hdi.t[1], digits), ", ", round(hdi.t[2], digits), "]", sep = "")

}

if(incl.mean == T) {

  if(wordy == T) {
  
  if(calculate == T) {hdi.text <- paste(round(mean(mcmc$mu), digits), ", 95\\% HDI [", round(hdi.t[1], digits), ", ", round(hdi.t[2], digits), "]", sep = "")}
  
  if(calculate == F) {hdi.text <- paste(round(mean(x), digits), ", 95\\% HDI [", round(hdi.t[1], digits), ", ", round(hdi.t[2], digits), "]", sep = "")}
  
  }
  
    if(wordy == F) {
    if(calculate == T) {hdi.text <- paste(round(mean(mcmc$mu), digits), ", [", round(hdi.t[1], digits), ", ", round(hdi.t[2], digits), "]", sep = "")}
  
  if(calculate == F) {hdi.text <- paste(round(mean(x), digits), ", [", round(hdi.t[1], digits), ", ", round(hdi.t[2], digits), "]", sep = "")}

}
}

return(hdi.text)
}


betabinom.hdi <- function(vec, 
                          alpha.prior = 1, 
                          beta.prior = 1, 
                          quant = .95, 
                          na.rm = T) {
  
  vec <- as.vector(vec)
  
  if(na.rm) {
    
    vec <- vec[is.finite(vec)]
    
  }
    
  
  n <- length(vec)
  successes <- sum(vec == 1)
  failures <- sum(vec == 0)
  
  alpha <- successes + alpha.prior
  beta <- failures + beta.prior
  
  lb <- qbeta((1 - quant) / 2, shape1 = alpha, shape2 = beta)
  ub <- qbeta(1 - ((1 - quant) / 2), shape1 = alpha, shape2 = beta)
  mode <- alpha / (beta + alpha)
  
  hdi.text <- paste(round(mode, 2), " (95\\% HDI = [", round(lb, 2), ", ", round(ub, 2), "])", sep = "")
  
  
  
  return(list("lb" = lb, "ub" = ub, "mode" = mode, "text" = hdi.text))
  
}
  
add.binom.p.bar <- function(vec, 
                            alpha.prior = 1, 
                            beta.prior = 1, 
                            quant = .95, 
                            bar.col, 
                            location = 1, 
                            bar.width = .5, 
                            add.text = T,
                            bar.label = ""
                            ) {
  
  
  lb <- betabinom.hdi(vec)$lb
  ub <- betabinom.hdi(vec)$ub
  
  rect(location - bar.width / 2, 0, location + bar.width / 2, mean(vec), col = bar.col)
  segments(location, lb, location, ub, lwd = 2)
  
  if(add.text) {
    
    text(location + bar.width / 2, mean(vec), labels = round(mean(vec), 2), pos = 4)
  }
  
  mtext(bar.label, side = 1, at = location)
  
    
  }


transparent <- function(orig.col = "red", trans.val = 1, maxColorValue = 255) {
    
    if(length(orig.col) == 1) {orig.col <- col2rgb(orig.col)}
    if(!(length(orig.col) %in% c(1, 3))) {return(paste("length of original color must be 1 or 3!"))}
    
    final.col <- rgb(orig.col[1], orig.col[2], orig.col[3], alpha = (1 - trans.val) * 255, maxColorValue = maxColorValue)
    
    return(final.col)
}

compare.Coins <- function(Heads.A, Flips.A, Heads.B, Flips.B, n.iter = 10000, n.burnin = 1000, n.thin = 1) {
  
  modelString = "
# JAGS model specification begins ...
model {

    Heads.A ~ dbin(p.A, Flips.A)
    Heads.B ~ dbin(p.B, Flips.B)


    # Prior distribution:
    p.A ~ dbeta(1, 1)
    p.B ~ dbeta(1, 1)

p.delta <- p.B - p.A

}
"

 
  
  writeLines(modelString, con = "model.txt")
    
    parameters <- c("p.A", "p.B", "p.delta")
    
    JAGS.Output <- jags(data = list("Heads.A", "Flips.A", "Heads.B", "Flips.B"),   
                        parameters.to.save = parameters,
                        model.file ="model.txt",
                        n.chains = 1, n.iter = n.iter, n.burnin = n.burnin, n.thin = n.thin)
    
 
  MCMC.All <- as.mcmc(JAGS.Output$BUGSoutput$sims.matrix) # unordered, pooled across chains
  Summary <- summary(MCMC.All)
  
 return(list("Summary" = Summary, "MCMC" = MCMC.All))
 
}
  
  
@


\maketitle

\section*{Introduction}

One day in the 1980s, a man named Martin Frankel walked into a stock broker's office in Toledo Ohio and said he wanted to invest in the stock market. Frankel explained to the owner of the office that after studying the market for years, he had develoepd a unique insight into the market -- to the point where he was cetain he could beat it. The owner was so impressed by Frankel's vision that he actually hired Frankel as a trader. Over the next 6 months, Frankel all-but lived at the broker's office -- meticulously studying the market and developing increasingly ingenious trade ideas. However, after 6 months, the owner became increasingly frustrated with Frankel. Despite all of his `brilliant' theories and plans, Frankel never actually \textit{acted} on any of his theories by making a trade. By his own admission, Frankel suffered from what he called `trader's block' wherein he had a hard time `pulling the trigger.' Because of his lack of action, Frankel was fired from the small broker's office \footnote{Frankel's story would have been forgotten if not for the fact that he  went on to earn over $200 Million dollars in a series of investment scams and Ponzi schemes. He is currently serving a XX year prison sentence.}.

Why did Frankel fail as a legitimate broker? On the one hand, patience is a virtue -- one that Frankel had in spades. If Frankel had acted more impulsively and invested all of the firm's money in one `hot' stock on his first day, he would may been fired for putting the entire firm at risk on too little knowledge. Instead, Frankel's failure as a legitimate stock broker exemplifies the cost of waiting too long to act.

This example exemplifies a common trade-off between acquiring information without risk, and acting on existing information with risk. This trade-off occurs in many areas of life. 

\begin{itemize}

\item Mate Selection: A person must decide how long to get to know someone via dating before committing to a long-term relationship. Too little dating could lead to marring a poor match, while waiting too long could lead to no marriage at all.

\item Food Foraging: 

\item Athletic training: How long should I train in boxing before I schedule my first real match?

\end{itemize}


\subsection{Exploration-exploitation trade-off}

Many of the most important real world decisions require individuals to reap consequences from several risky options that probabilistically give rewards and punishments. In many tasks, these decisions are made under uncertainty, where the probabilities and magnitudes associated with options are \textit{a priori} unknown. In order to learn about options, organisms can engage in active search which improves the quality of their impressions of options. However, search can come at a cost, such as the missed opportunity to receive rewards from known options. For example, in trying a new restaurant, one forgoes the opportunity to have a meal at her (current) favorite restaurant.

This conflict between obtaining new information and acting on existing information is known as the exploration-exploitation trade-off. The exploration-exploitation (EE) trade-off is one of the most widely studied aspects of decision making from human to non-human organisms. The exploration-exploitation trade-off represents a goal conflict in decisions under uncertainty, where an organism is trying to maximize its long term rewards from \textit{a priori} unknown options. On the one hand, individuals want to explore options by gaining as much information as possible to improve the quality of their future decisions. On the other hand, they want to \textit{exploit} options by acting on existing information in order to increase short-term rewards.

One of the most widely used experimental tasks used to study the exploration-exploitation conflict is the n-armed bandit. In an n-armed bandit, participants have a fixed number of trials to select an option and experience a consequential reward.


\subsection{Mode Switching}

Other research that has studied elective mode switching

\begin{itemize}

  \item \cite{halberstadt1995transitions}: Passive observation versus active experimentation in rule learning. Participants can freely choose between observation and experimental manipulation. ``With such a paradigm it will be possible to observe whether either mode of inquiry is preferred over the other at some or all stages of the inquiry process'' (p. 284). Result 1: People tend to passively observe prior to actively experiment. Result 2: Modes come in `clumps' - people tend to stick to one mode. Result 3: The environment affects how people generate hypotheses.
  
  

\end{itemize}


\subsection{Purely epistemic versus pragmatic actions}

\begin{itemize}

  \item \cite{neth2008thinking} distinguished between two types of actions, epistemic and pragmatic. Epistemic actions are those that result in information rather than punishments or rewards, while pragmatic actions are those that lead to punishments or rewards. Exploration is assumed to be an epistemic action while exploitation is a pragmatic action.
  
  \item One can easily imagine real-world cases where people explicitly engage in purely epistemic actions. For example, imagine a person who wishes to learn about the stock market prior to risking any real money. He can do this by viewing sequential returns from several stocks and observing their risk. Alternatively, a new resident to a town can learn about local restaurants by asking her neighbors about their recent experiences. In all of these cases, the actor is learning about options without reaping consequences.
  
  \item Clearly these purely epistemic actions are both psychologically and behaviorally distinct from pragmatic actions, where one obtains \textit{both} information and immediate consequences. For example, our stock investor who starts investing his money into stocks will then not only learn about their performance, but also reap gains and suffer consequences. Similarly, the new town resident who starts frequenting local restaurants will continue learning about them but also experience immediate pragmatic outcomes.
  
  \item Somewhat puzzlingly, paradigms that have been used to study exploration-exploitation trade-off in humans has largely ignored behavioral differences in epistimic and pragmatic actions. In the N-armed bandit task, players are only allowed to engage in one type of behavior - choice, which always provides both epistemic and pragmatic rewards. Players are not given the option to engage in purely epistemic actions. 
  
  \item This can lead to erroneous inferences. The same choice behavior could be interpreted as either resulting from an epistemic or pragmatic motivation. Until now, researchers have had to use computational cognitive modeling techniques to attribute choices post-hoc to either an epistemic or pragmatic underlying goal.
  
  \item We believe a new paradigm is needed. One where individuals have the option to explicitly explore or exploit options. This task will not only be a better model of many real-world decision tasks than previous paradigms, but will also allow researchers to explicitly observe behavior consistent with purely epistemic goals.

\end{itemize}

\subsection{Combining three paradigms}

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Paradigm & EE Tradeoff & Pure Exploration & Pure Exploitation & Alternation \\ \hline
    N-Armed Bandit & Yes & No & No & Yes\\ \hline
    Sampling Paradigm & No & Yes & Yes & No\\ \hline
    Bet-Observe & Yes & Yes & Yes & Yes\\ \hline
    Peeks and Keeps & Yes & Yes & No & Yes\\ \hline
    \hline
    \end{tabular}
\end{center}


In a multi-armed bandit task, participants choose between multiple a priori unknown options over several trials and receive rewards (or costs) on each trial. Because decision makers reap consequences on every trial, the n-armed bandit task does not allow purely epidemic actions. The Iowa Gambling Task (IGT) is one famous example of this paradigm. Using cognitive models such as the expectancy-valence model, researchers have used the IGT to study cognitive mechanisms such as loss-aversion, recency, and choice consistency in both healthy and non-healthy individuals \citep{yechiam2005models}.

Two paradigms have been used to study purely epistemic actions: the sampling paradigm of decisions from experience \citep{hertwig2004decisions} and the bet-observe task \citep{tversky1966information}. Like the n-armed bandit task, both paradigms present participants with multiple, a priori unknown options. In the sampling paradigm, participants can then sample from options, without consequence, as many times as they would like before making a single consequential choice. Here, participants engage in a self-determined number of purely epistemic actions strictly prior to a single purely pragmatic action. After making their final choice, participants receive the consequences from their choice but cannot continue to observe. Thus, in the sampling paradigm observation strictly occurs prior to exploitation with no possibility to alternate between the two modes.

As far as we know, the bet-observe task is the only paradigm that allows individuals to alternate between pure exploration and pure exploitation. In the bet-observe task, an individual is presented with two options. On each of M trials, one of the two options will produce a reward indicated by a green light. On each trial, the participant selects an option and makes one of two choices. He can \textit{observe} an option, see which one produces the reward, but not receive the reward. Or he can \textit{bet} on an option. If the player bets on an option, he will gain its underlying reward but will not see whether the reward is present. Because the participant only sees the option outcome if he observes, he can only learn about the options' underlying distributions on observation trials, but can only reap rewards on betting trials.

Navarro and Newell (2014) derived optimal decision strategies for two versions of the game: stationary and non-stationary. In the stationary version of the game, the reward probability distributions are fixed. Specifically, the probability that the left option has a reward $l_{p}$ is fixed and does not change over time. In the stationary task, an optimal learner will begin the task by observing outcomes until he reaches a pre-defined information threshold. Once he reaches this threshold, he will switch to a betting strategy and will always bet on the perceived better option. In the non stationary version of the game, the reward probability distributions can change at any time. For example, with some probability $\alpha$ the probability $l_{p}$ could change to a value drawn from a uniform distribution. In this version of the game, the optimal decision strategy alternates between observing and betting throughout the game. In other words, the actor will begin by observing for a few trials until a certain information threshold is reached, then he will switch to betting for a few trials. He will then switch to observing in order to see if $l_{p}$ has changed.

However, because betting in the bet-observe task does not provide information, decision makers cannot learn anything on betting trials. This is not an inherent flaw in the paradigm - indeed, obscuring information from betting trials elegantly separates epistemic from pragmatic actions. However, because many, if not most, real-world decision tasks provide information on both exploration and exploitation trials, the bet-observe task is a poor model of most real-world decisions. From food choice to mate choice, exploitation decisions (i.e.; consuming food or selecting a mate) will always provide information to the decision maker that it can use to update its impressions and guide future search. 

In order to study how people alternate between explicit exploration and exploitation, we introduce the Peeks and Keeps task.


\section{Peeks and Keeps}

Peeks and Keeps is an extension of an N-armed bandit task that explicitly separates exploration and exploitation decisions. In the task, participants repeatedly select one of N options with \textit{a priori} unknown underlying probability distributions over the course of M trials. On each trial, the participant selects an option and elects to either \textit{observe} the next outcome without financial feedback, or \textit{bet} on the outcome and receive the financial feedback. At the end of M trials, the participant is paid the sum of all sample outcomes revealed on bet trials. If he always observes and never bets, he receives no bonus. If he bets on every trial, he receives the sum total of all samples.


\section{``Optimal'' Search in Peeks and Keeps}

How many peeks \textit{should} people take when playing peeks and keeps? The answer to this question depends on two critical criteria: the specific search strategy a person uses, and the statistical environment they are in. With regards to search strategies, 
we assume that people use a simple ``explore equally then exploit'' strategy. This strategy assumes that people begin by exploring the environment equally across options using a pre-defined number of peeks. Once the person has used all of their peeks, they shift to an epsilon-greedy exploitation strategy.

In the following simulation, we will focus on the effect of statistical environments on optimal search strategies. Before going into the details of the simulation, we note that it is easy to derive trivial environments that would prescribe either the minimum (i.e.; 0) or maximum (i.e.; Infinite) peeks. An environment with options that only provide positive outcomes prescribes 0 peeks, while and environment with options that only provide negative outcomes prescribes infinite peeking. These trivial boundary conditions already suggest that there are a range of intermediate environments that prescribe intermediate levels of observation.

To reduce the strategy and environmental space in this simulation, we will make several restrictions. We assume that each environment has one option with a positive expected value, and one (or more) options with a negative expected value.

We varied three parameters in our simulation, one at the agent level, and two at the environment level.

\subsubsection{Agent Parameters}

\begin{enumerate}

  \item Number of Peeks: The number of peeks agents used ranged from 0 to 100 in steps of 5.

\end{enumerate}
  
  
\subsubsection{Environment Parameters}

\begin{enumerate}

  \item Number of negative EV options: The number of negative EV options ranged from 1 to 3. Because there was always one positive-EV option, the number of total options in the environments ranged from 2 to 4.
  \item Standard deviation of option distributions: We used three different standard deviations of option distributions: 5, 15, and 60. In each environment, the standard deviation of all options (both positive-EV and negative-EV) was the same.

\end{enumerate}

We had 5,000 agents play the game for each parameter combination. In Figure~\ref{fig:optimal}, we plot the environments and the median number of points earned by 5,000 agents using 0 to 100 peeks:

% Plot optimal peeks simulation
<<echo = F, eval = T, fig.width = 8, fig.height = 4, message = F, results = 'hide'>>=
{
   
col.vec <- piratepal("google")
  
points.agg <- read.table("simulations/oct 1 2015/oct 1 2015 agg.txt")

# Plotting

pdf("figures/optimal_result_points.pdf", width = 11, height = 5)


n.bad.vec <- sort(unique(points.agg$n.bad))
sd.vec <- sort(unique(points.agg$sd))

par(mfrow = c(1, 4))
par(mar = c(3, 5, 5, 1))

  for(sd.i in 1:length(sd.vec)) {
    
    plot(1, 
         xlim = c(-5, 100), 
         ylim = c(-100, 500), 
         type = "n",
         ylab = "Mean points earned",
         xlab = "Number of peeking trials",
         main = paste("sd = ", sd.vec[sd.i], sep = ""),
         cex.main = 1
         )
    
    mtext("Pos ~ N(5, sd), Neg ~ N(-5, sd)", side = 3, line = .2, cex = .5)
    
    rect(-10000, -1000, 10000, 10000, col = gray(.95))
    
    abline(h = seq(-1000, 1000, 100), col = gray(1), lwd = 1.5)
    abline(h = seq(-1050, 1050, 100), col = gray(1), lwd = .75)
    
    abline(v = seq(0, 100, 10), col = gray(1), lwd = 1.5)
    abline(v = seq(50, 150, 10), col = gray(1), lwd = .75)
    
    abline(h = 0)
    
    legend("topright",
           paste("N neg = ", n.bad.vec, sep = ""),
           pch = 16,
           col = col.vec,
           lty = 1
           )
    
    
  for(n.bad.i in 1:length(n.bad.vec)) {
    
    data <- subset(points.agg, n.bad == n.bad.vec[n.bad.i] & sd == sd.vec[sd.i])
    
    lines(data$n.peeks, 
          data$total.points.mean, 
          col = col.vec[n.bad.i], type = "b", pch = 16)
    
    optimal.peeks <- min(data$n.peeks[data$total.points.mean == max(data$total.points.mean)], na.rm = T)
    optimal.points <- max(data$total.points.mean, na.rm = T)
    
    points(optimal.peeks, optimal.points, pch = 21, cex = 1.5)
    text(optimal.peeks, 
         optimal.points, 
         labels = paste(optimal.peeks, ", ", round(optimal.points, 0), sep = ""), 
         pos = 3)
    
    # lines(subset(data, select.strategy == "egreedy.25")$n.trials, 
    #       subset(data, select.strategy == "egreedy.25")$impression.mad, col = "blue")
    # 
    # lines(subset(data, select.strategy == "egreedy.05")$n.trials, 
    #       subset(data, select.strategy == "egreedy.05")$impression.mad, col = "blue")
    

}

}

dev.off()

}




@


\begin{figure}
\centering
\includegraphics[width=6in]{figures/optimal_result_points.pdf}
\caption{\label{fig:optimal}Aggregate results from agent-based sampling simulation. All agents began the game by peeking equally between all options until reaching a pre-determined number of peeks. Once they finished using their peeks, agents exclusively used keeps with an epsilon-greedy rule with a 0.05 probability of randomly choosing an option that was not the current best option. Each point in the plot represents the mean number of points earned by 5,000 agents playing the game with a given number of peeking trials (x-axis) in different problem environments (different panels). Each environment had one option with a positive EV (equal to +5) and one or more options (different panel rows) with negative EV (-5). All options were normally distributed; however, the standard deviation of options differed between environments (different panel columns).}
\end{figure}


<<echo = F, results = 'asis', message = F>>=

optimal.ss.table <- expand.grid("n.bad" = unique(points.agg$n.bad),
                                "sd" = unique(points.agg$sd),
                                "npeeks" = NA, "max.points" = NA
                                )

for (i in 1:nrow(optimal.ss.table)) {
  
  n.bad.i <- optimal.ss.table$n.bad[i]
  sd.i <- optimal.ss.table$sd[i]
  
  data.temp <- subset(points.agg, n.bad == n.bad.i & sd == sd.i)
  
  max.points.i <- max(data.temp$total.points.mean, na.rm = T)
  npeeks.i <- data.temp$n.peeks[data.temp$total.points.mean == max.points.i & is.finite(data.temp$total.points.mean)]
  
  optimal.peek.impression.mad <- data.temp$final.peek.impression.mad[data.temp$total.points.mean == max.points.i]
  
  optimal.final.peek.prefer.best.mean <- data.temp$final.peek.prefer.best.mean[data.temp$total.points.mean == max.points.i]
  
  
    optimal.peek.impression.mad <- data.temp$final.peek.impression.mad[data.temp$total.points.mean == max.points.i & is.finite(data.temp$total.points.mean)]
  
  optimal.final.peek.prefer.best.mean <- data.temp$final.peek.prefer.best.mean[data.temp$total.points.mean == max.points.i & is.finite(data.temp$total.points.mean)]
  
  final.impression.mad <- data.temp$final.impression.mad[data.temp$total.points.mean == max.points.i & is.finite(data.temp$total.points.mean)]
  
  
  optimal.ss.table$npeeks[i] <- npeeks.i
  optimal.ss.table$max.points[i] <- max.points.i
  optimal.ss.table$peek.MAID[i] <- optimal.peek.impression.mad
  optimal.ss.table$peek.PB[i] <- optimal.final.peek.prefer.best.mean
  optimal.ss.table$end.MAID[i] <- final.impression.mad 
}  

optimal.ss.table.x <- xtable(optimal.ss.table, 
                               caption = "Optimal number of peeks (and resulting expected number of points) in simulation 1", 
                               label = "table:optimalsim1")

print(optimal.ss.table.x, include.rownames = F,
      sanitize.text.function = function(x){x},  
      floating.environment = getOption("xtable.floating.environment", "table*"))
@

The optimal number of peeks and their associated expected point earnings for each environment is presented in Table~\ref{table:optimalsim1}. Here, we see that as both the number of bad options and standard deviation of option outcomes increases, the optimal number of peeks increases. In the easiest environment, with 1 negative option and an option standard deviation of 5, the optimal number of peeks is \Sexpr{subset(optimal.ss.table, n.bad == 1 & sd == 5)$npeeks} leading to an expected earning of \Sexpr{subset(optimal.ss.table, n.bad == 1 & sd == 5)$max.points} points. This suggests that this environment is so easy to learn that peeking is unnecessary, and even detrimental. In contrast, in the most difficult environment, with 3 negative outcomes and an option standard deviation of 30, the optimal number of peeks is \Sexpr{subset(optimal.ss.table, n.bad == 3 & sd == 30)$npeeks} leading to an expected earning of \Sexpr{subset(optimal.ss.table, n.bad == 3 & sd == 30)$max.points} points.


%' \subsection{Required learning in each environment}
%' 
%' How much learning is necessary in each environment? To answer this, we calculated how well agents using the optimal number of peeks learned their environments. We defined learning with two measures: mean absolute impression deviation (MAID), the mean absolute difference between agent's impressions of options and the option's true EV, and prefer best (PB), the probability that, at the end of its peeking trials, the agent preferred the best option. 
%' 
%' <<eval = F, echo = F>>=
%' 
%' # Load and aggregated simulations
%'   
%' date.to.use <- "oct 3 2015"
%' 
%' 
%' {
%'   
%' files.to.use  <- list.files(paste("simulations/", date.to.use, " learning", sep = ""))
%' files.to.use <- files.to.use[grepl("result", files.to.use)]
%' 
%' 
%' for(i in 1:length(files.to.use)) {
%'   
%'   current.df <- read.table(paste("simulations/", date.to.use, " learning/", files.to.use[i], sep = ""), 
%'                            stringsAsFactors = F, 
%'                            header = T, 
%'                            sep = "\t")
%'   
%'   if(i == 1) {peeks.sim <- current.df}
%'   if(i > 1) {peeks.sim <- rbind(peeks.sim, current.df)}
%'   
%' }
%' 
%' ## Aggegate data
%' learning.agg <- peeks.sim %>%
%'   group_by(n.bad, sd, n.peeks) %>%
%'   summarise(
%'     final.impression.mad.mean = mean(final.impression.mad),
%'     final.peek.prefer.best.mean = mean(final.peek.prefer.best, na.rm = T),
%'     n = n()
%'     )
%' 
%' 
%' write.table(learning.agg, paste("simulations/", date.to.use, " learning/", date.to.use, " agg.txt", sep = ""), sep = "\t")
%' }
%' 
%' 
%' @
%' 
%' <<echo = F, message = F, prompt = F>>=
%' 
%' date.to.use <- "oct 3 2015"
%' 
%' learning.agg <- read.table(paste("simulations/", date.to.use, " learning/", date.to.use, " learning agg.txt", sep = ""), sep = "\t")
%' 
%' 
%' pdf("figures/optimal_result_learning.pdf", width = 11, height = 5)
%' {
%' n.bad.vec <- sort(unique(learning.agg$n.bad))
%' sd.vec <- sort(unique(learning.agg$sd))
%' 
%' par(mfrow = c(1, 4))
%' par(mar = c(3, 5, 5, 1))
%' 
%'   for(sd.i in 1:length(sd.vec)) {
%'     
%'     plot(1, 
%'          xlim = c(-5, 100), 
%'          ylim = c(0, 1), 
%'          type = "n",
%'          ylab = "p(prefer good option)",
%'          xlab = "Number of peeking trials",
%'          main = paste("sd = ", sd.vec[sd.i], sep = ""),
%'          cex.main = 1
%'          )
%'     
%'     mtext("Pos ~ N(5, sd), Neg ~ N(-5, sd)", side = 3, line = .2, cex = .5)
%'     
%'     rect(-10000, -1000, 10000, 10000, col = gray(.95))
%'     
%'     abline(h = seq(0, 1, .1), col = gray(1), lwd = 1.5)
%'     abline(h = seq(.05, 1.05, .1), col = gray(1), lwd = .75)
%'     
%'     abline(v = seq(0, 100, 10), col = gray(1), lwd = 1.5)
%'     abline(v = seq(50, 150, 10), col = gray(1), lwd = .75)
%'     
%'     abline(h = 0)
%'     
%'     legend("bottomright",
%'            paste("N neg = ", n.bad.vec, sep = ""),
%'            pch = 16,
%'            col = col.vec,
%'            lty = 1
%'            )
%'     
%'     
%'   for(n.bad.i in 1:length(n.bad.vec)) {
%'     
%'     data <- subset(learning.agg, n.bad == n.bad.vec[n.bad.i] & sd == sd.vec[sd.i])
%'     
%'     
%'     lines(data$n.peeks, 
%'           data$final.peek.prefer.best.mean, 
%'           col = col.vec[n.bad.i], type = "b", pch = 16)
%'     
%' #     optimal.peeks <- min(data$n.peeks[data$total.points.mean == max(data$total.points.mean)])
%' #     optimal.points <- max(data$total.points.mean)
%' #     
%' #     points(optimal.peeks, optimal.points, pch = 21, cex = 1.5)
%' #     text(optimal.peeks, 
%' #          optimal.points, 
%' #          labels = paste(optimal.peeks, ", ", round(optimal.points, 0), sep = ""), 
%' #          pos = 3)
%'     
%'  
%' 
%' }
%' 
%' }
%' 
%' dev.off()
%' }
%' 
%' 
%' @
%' 
%' 
%' \begin{figure}
%' \centering
%' \includegraphics[width=6in]{figures/optimal_result_learning.pdf}
%' \caption{\label{fig:learning}Aggregate results from agent-based sampling simulation. learning results.}
%' \end{figure}
%' 




% Load Data


<<echo = F, message = F, warning = F>>=
participants <- read.table("data/participants.txt", sep = "\t", header = T)
actions <- read.table("data/actions.txt", sep = "\t", header = T)
condition.table <- read.table("data/conditiontable.txt", sep = "\t", header = T)
distributions <- read.table("data/oct2015/peekkeep_oct2015_gamedist.txt", 
                            sep = "\t", 
                            header = T)

  optionorder <- read.table("data/oct2015/peekkeep_oct2015_optionorder.txt", 
                            sep = "\t", 
                            header = T)

@




% Plotting and analysis functions

<<echo = F>>=
plot.pak <- function(n.options,
                             worker.name = "",
                             game.type = "",
                             option.sd = "",
                             selection.ls,
                             outcome.ls,
                             action.ls,
                             best.option.vec,
                             col.vec = piratepal("google", length.out = 6)
                             ) {
  
  games.to.plot <- length(selection.ls)
  
if(length(col.vec) < 6) {col.vec <- rep(col.vec, length.out = 6)}
  
if(games.to.plot == 2) {

layout(matrix(c(1, 1, 2, 3), nrow = 2, ncol = 2, byrow = T), 
       widths = c(4, 4), heights = c(.85, 4))
  
}

if(games.to.plot == 1) {

layout(matrix(c(1, 2), nrow = 2, ncol = 1, byrow = T), 
       widths = c(4), heights = c(.85, 4))
  
}


# Top plot

par(mar = rep(0, 4))

plot(1, xlim = c(0, 1), ylim = c(0, 1), 
     xaxt = "n", yaxt = "n", xlab = "", ylab = "", type = "n", bty = "n")

text(rep(.5, 4), 
     c(.7, .45, .25, .05), 
     c(worker.name, paste("Type =", game.type), 
       paste("N =", n.options), 
       paste("sd =", option.sd)),
     cex = c(2, 1.5, 1.5, 1.5)
     )
     
#paste(worker.i, "\n", game.type.i, ", n = ", n.options.i, ", sd = ", option.sd.i, sep = ""), cex = 2)

par(mar = c(5, 4, 4, 1) + .1)


for(game.i in 1:games.to.plot) {

  selection.vec <- selection.ls[[game.i]]
  outcome.vec <- outcome.ls[[game.i]]
  action.vec <- action.ls[[game.i]]
  best.option <- best.option.vec[game.i]
  
  n.trials <- length(selection.vec)
  
  reward.vec <- outcome.vec
  reward.vec[action.vec == "peek"] <- 0
  
  cumreward.vec <- cumsum(reward.vec)
  
  peekreward.vec <- outcome.vec
  peekreward.vec[action.vec == "keep"] <- 0
  cumpeekreward.vec <- cumsum(peekreward.vec)
  
  select.best.vec <- selection.vec == best.option
  
  option.switch.vec <- c(NA, selection.vec[2:n.trials] != selection.vec[1:(n.trials - 1)])
  
   action.switch.vec <- c(NA, action.vec[2:n.trials] != action.vec[1:(n.trials - 1)])
  
  
par(mar = c(5, 5, 4, 3))

plot(1, xlim = c(0, 120), ylim = c(-500, 500), type = "n", 
     main = paste("Game", game.i), 
     ylab = "Cumulative Points", xlab = "Trial", 
     xaxt = "n", yaxt = "n", bty = "n", cex.main = 1.5)


par(xpd = NA)

segments(20, 510, 40, 510, col = col.vec[2], lwd = 2)
text(20, 530, "Peeks", adj = 0)

segments(60, 510, 80, 510, col = col.vec[1], lwd = 2)
text(60, 530, "Keeps", adj = 0)

par(xpd = FALSE)

axis(1, at = seq(0, 100, 20))
axis(2, at = seq(-500, 500, 100), las = 1)

# Add margin text for final points

text(103, cumreward.vec[length(cumreward.vec)], 
    cumreward.vec[length(cumreward.vec)], adj = 0)


text(103, cumpeekreward.vec[length(cumpeekreward.vec)], 
      cumpeekreward.vec[length(cumpeekreward.vec)], adj = 0)


h.line.loc <- seq(-500, 500, 100)
v.line.loc <- seq(0, 100, 20)

segments(rep(0, length(h.line.loc)), 
         h.line.loc, 
         rep(100, length(h.line.loc)),
         h.line.loc, col = gray(.9)
)

segments(
         v.line.loc, 
         rep(-500, length(v.line.loc)), 
         v.line.loc, 
         rep(500, length(v.line.loc)),
         col = gray(.9)
)

abline(h = 0, col = "red", lty = 2)

lines(1:100, cumreward.vec, col = col.vec[1], lwd = 2)
lines(1:100, cumpeekreward.vec, col = col.vec[2], lwd = 2)

peek.trials <- which(action.vec == "peek")
keep.trials <- which(action.vec == "keep")

pch.vec <- rep(21, n.trials)
#pch.vec[game.data$option.switch == TRUE] <- 24

# Add peek symbols

points(peek.trials, 
       cumpeekreward.vec[peek.trials], 
       pch = pch.vec[peek.trials], 
       bg = transparent(col.vec[2], .6), 
       cex = 1.2, 
       col = gray(.3))

# Add keep symbols
points(keep.trials, 
       cumreward.vec[keep.trials], 
       pch = pch.vec[keep.trials], 
       bg = transparent(col.vec[1], .6), 
       cex = 1.2, 
       col = gray(.3))


# Add select best line

text(102, -195, "Select Best", adj = 0)
select.best.trials <- which(select.best.vec == TRUE)
rect(select.best.trials - .5, 
     rep(-200, length(select.best.trials)),
     select.best.trials + .5,
     rep(-190, length(select.best.trials)),
     col = col.vec[4], lwd = .5
     )

# Add option switch line

text(102, -245, "Option Change", adj = 0)
option.switch.trials <- which(option.switch.vec == TRUE)
rect(option.switch.trials - .5, 
     rep(-250, length(option.switch.trials)),
     option.switch.trials + .5,
     rep(-240, length(option.switch.trials)),
     col = col.vec[5], lwd = .5
)


# Add action switch line

text(102, -295, "Action Change", adj = 0)
action.switch.trials <- which(action.switch.vec == TRUE)
rect(action.switch.trials - .5, 
     rep(-300, length(action.switch.trials)),
     action.switch.trials + .5,
     rep(-290, length(action.switch.trials)),
     col = col.vec[6], lwd = .5
)


# Add mean totals
# {
# bar.min <- -450
# bar.height.max <- 200 
# 
# rect(0, bar.min, 30, bar.min + bar.height.max, col = "white", border = NA)
# 
# rect(0, bar.min, 10, 
#      bar.min + sum(game.data$mode == "peek") / 100 * bar.height.max)
# 
# text(5, bar.min + sum(game.data$mode == "peek") / 100 * bar.height.max,
#      sum(game.data$mode == "peek"), pos = 3)
# 
# text(5, bar.min, labels = "p", pos = 1)
# 
# rect(10, bar.min, 20,
#      bar.min + sum(game.data$option.switch, na.rm = T) / 100 * bar.height.max)
# 
# text(15, bar.min + sum(game.data$option.switch, na.rm = T) / 100 * bar.height.max,
#      sum(game.data$option.switch, na.rm = T), pos = 3)
# 
# text(15, bar.min, labels = "os", pos = 1)
# 
# rect(20, bar.min, 30,
#      bar.min +  sum(game.data$mode.switch, na.rm = T) / 100 * bar.height.max)
# 
# text(25, bar.min + sum(game.data$mode.switch, na.rm = T) / 100 * bar.height.max,
#      sum(game.data$mode.switch, na.rm = T), pos = 3)
# 
# text(25, bar.min, labels = "ms", pos = 1)
# }

}

}
@



\section{Questions}

\begin(itemize)

  \item Do people benefit from peeks or not?
  \item In PAK, do people alternate or do they utilize a PTK strategy?

\end{itemize}


\section{Method}

\subsection{Participants}

Participants (N = \Sexpr{nrow(participants)}) were recruited from the Amazon Mechanical Turk\footnote{We restricted our study to workers who had completed at least 100 HITs with at least a 95\% HIT acceptance rate.}. For their participation, workers received a guaranteed reward of 50 cents with the possibility of a bonus up to \$1.00. \Sexpr{sum(participants$sex == 2)} (\Sexpr{round(mean(participants$sex == 2), 2) * 100}\%) were female and the mean age was (\Sexpr{round(mean(participants$age), 2)}) (IQR: [\Sexpr{round(quantile(participants$age, .25), 2)}, \Sexpr{round(quantile(participants$age, .75), 2)}])

\subsection{Procedure}

We created four different environments by crossing two factors: Number of options (2 v. 4), and Option standard deviation (5 v. 20)\footnote{In order to ensure that the sample distributions closely matched the desired means and standard deviations, we repeatedly generated candidate sample distributions until we found ones whose sample means were within 0.10 of the desired mean and whose standard deviations were within 1.0 of the desired value. Additionally, we truncated the distributions so the minimum and maximum values did not exceed -99 and +99 respectively.}. The four environments are displayed in Figure \ref{fig:distributions}. The order of options was randomized for each participant.

% distributions.pdf
<<echo = F, results = "hide">>=

pdf("figures/distributions.pdf", width = 10, height = 7)

par(mfrow = c(2, 2))
par(mar = c(4, 3, 4, 1))

option.vec <- c(1, 3, 1, 3)
sd.vec <- c(5, 5, 20, 20)

col.vec <- piratepal("google", length.out = 4)[c(4, 2, 1, 3)]
ball.col.vec <- piratepal("google", length.out = 4, trans = .4)[c(4, 2, 2, 2)]
ball.outline.vec <- piratepal("google", length.out = 4, trans = 0)[c(4, 2, 2, 2)]


for(i in 1:4) {
  
  options.i <- option.vec[i]
  sd.i <- sd.vec[i]
  
  
  if(sd.i == 5) {plot.height <- .1 ; ball.height <- .11 ; ball.sd <- .007 ; text.height <- .15 ; line.width <- .02}
  if(sd.i == 20) {plot.height <- .03 ; ball.height <- .032 ; ball.sd <- .0025 ; text.height <- .0475 ; line.width <- .02}
  
  good.pdf <- function(x) {return(dnorm(x, mean = 5, sd = sd.i))}
  bad.pdf <- function(x) {return(dnorm(x, mean = -5, sd = sd.i))} 
  bad2.pdf <- function(x) {return(dnorm(x - 2, mean = -5, sd = sd.i))} 
  bad3.pdf <- function(x) {return(dnorm(x + 2, mean = -5, sd = sd.i))} 
  
  plot(1, xlim = c(-50, 50), ylim = c(0, plot.height), xlab = "", ylab = "", main = "", yaxt = "n")

  curve(good.pdf, from = -50, to = 50, col = col.vec[1], lwd = 3, ylim = c(0, .12), add = T)
  
  curve(bad.pdf, from = -50, to = 50, col = col.vec[2], lwd = 3, ylim = c(0, .1), add = T)
  
  
  if (i %in% c(2, 4)) {
    
    curve(bad2.pdf, from = -50, to = 50, col = col.vec[3], lwd = 3, ylim = c(0, .1), add = T)
    curve(bad3.pdf, from = -50, to = 50, col = col.vec[4], lwd = 3, ylim = c(0, .1), add = T)
    
  }
  
  
  segments(0, 0, 0, 100, lty = 2)
  


  
  mtext(paste("Env", i), side = 3, line = 2, cex = 1.5)
  mtext(paste("N = ", option.vec[i] + 1, ", sd = ", sd.vec[i], sep = ""), side = 3, line = .5)
#  mtext("Density", side = 2, srt = 90, line = .5, cex = .8)
  mtext("Outcome", side = 1, line = 2.5, cex = .8)
  
}
  
  dev.off()

@

\begin{figure}
\includegraphics[width=\columnwidth]{figures/distributions.pdf}
\caption{4 decision environments used in Study 1. Each environment has one good option (in blue) with a positive expected value of +5. All other (bad) options have a negative expected value of -5.}
\label{fig:distributions}
\end{figure}


Each participant was randomly assigned to one of the 12 conditions (Response Mode (K v. PAK v. PTK) x Environment (1 v 2 v 3 v 4). In all response mode conditions, participants were told that the goal of the game was to earn as many points as possible over the course of 100 trials. To reinforce the idea that peeking introduces an opportunity cost, Those in the Peeks condition were specifically told that using a Peek action would `use a trial.'

After completing the first game, participants were spontaneously given a new game to play. They were told that the options in the second game were identical to those in the first game, but that the location of the options would be randomly shuffled. Participants completed three personality questionnaires (the XX, YYY, and the ZZZ) and an additional post-study questionnaire that elicited their overall impressions of the game.

\section{Results}

\subsection{Final Points}

Distributions of final points values across all games are presented in Figure \ref{fig:pointbeans}.

% pointbeans.pdf
<<echo = F, message = F, results='hide'>>=
# --------------------------------
#  Final point distributions by condition(s)
# --------------------------------

pdf("figures/pointbeans.pdf", width = 12, height = 7)
{

bean.locations <- c(1:3, 4:6 + .5, 7:9 + 1, 10:12 + 1.5)
  
  par(mar = c(1, 6, 8, 1))  
  col.vec <- rep(piratepal(palette = "google", trans = 0, length.out = 3), 4)
  

  plot(1, xlim = c(0, 14.5), ylim = c(-500, 600), xaxt = "n", xlab = "", type = "n", yaxt = "n", ylab = "")
  
  mtext(text = rep(c("Keep", "PAK", "PTK"), 4), at = bean.locations, side = 3, line = .5, cex = 1.3)
  mtext(text = paste("Env", 1:4), at = bean.locations[c(2, 5, 8, 11)], side = 3, line = 4, cex = 2)
 mtext(text = paste(c("N = 2, sd = 5", "N = 2, sd = 20", "N = 4, sd = 5", "N = 4, sd = 20")), at = bean.locations[c(2, 5, 8, 11)], side = 3, line = 2, cex = 2)
    
    
  mtext("Points Earned", side = 2, las = 0, line = 3, cex = 1.5)
  
  axis(side = 2, labels = seq(-500, 500, 500), at = seq(-500, 500, 500), las = 1, cex.axis = 1.2)
  
  rect(-1000, -1000, 1000, 1000, col = gray(.99), border = NA)
  abline(h = seq(-500, 500, 100), col = gray(.9), lwd = c(1, 3))
  abline(h = 0, col = "red", lwd = 2)
  
  
    beanplot(g1.final.points ~ game.type + n.options + option.sd, 
             data = subset(participants, is.finite(g1.final.points)), beanlines = "median", 
             ylim = c(-500, 600), main = "", 
             col = lapply(1:12, FUN = function(x) {c(transparent(col.vec[x], .8), gray(0), "white", "black")}), 
             at = bean.locations, xaxt = "n", cex.main = 1.5,
             beanlinewd = 5, what = c(0, 1, 1, 1), cutmax = 510, add = T, maxwidth = 1)
    
    
    medians <- aggregate(g1.final.points ~ game.type + n.options + option.sd, FUN = median, 
                         data = subset(participants,is.finite(g1.final.points)))
    
   # rect(bean.locations - .3, medians[,4] + 20, bean.locations + .3, medians[,4] + 120, col = gray(1, alpha = .8), border = NA)
    
    text(x = bean.locations, y = 525, pos = 3, labels = round(medians[,4], 0), cex = 1.5)
 
    
  }
  
dev.off()
@


\begin{figure}
\includegraphics[width=\columnwidth]{figures/pointbeans}
\caption{Distributions of final points earned by participants in each decision environment and experimental condition. Median point values are indicated by the dark black lines and are printed above each distribution.}
\label{fig:pointbeans}
\end{figure}

To see which game and environmental factors affected point totals, we calculated a Bayesian ANOVA using the BayesFactor package in R.

<<echo = F, results = 'hide', message = F>>=

participants$n.options.f <- as.factor(participants$n.options)
participants$option.sd.f <- as.factor(participants$option.sd)
participants$game.type.f <- as.factor(participants$game.type)
participants$environment.f <- as.factor(participants$environment)


# Version 1: All Data

points.frequentist <- lm(g1.final.points ~ game.type.f * n.options.f * option.sd.f,
                     data = subset(participants, is.finite(g1.final.points))
                     )

points.ao.bf <- anovaBF(g1.final.points ~ game.type.f * n.options.f * option.sd.f,
                     data = subset(participants, is.finite(g1.final.points)), whichModels = "top"
                     )

points.lm.bf <- lmBF(g1.final.points ~ game.type.f + n.options.f + option.sd.f + game.type.f : n.options.f,
                     data = subset(participants, g1.final.points > 0)
                     )

points.lm.bf.post <- posterior(points.lm.bf, iterations = 1e5)

points.bf.df <- extractBF(points.ao.bf)
row.names(points.bf.df) <- NULL
points.bf.df$Factor <- c("Environment X Gamee Type X Options",
                      "Game Type X SD",
                      "Game Type X Options",
                      "Options X SD",
                      "Game Type",
                      "Options",
                      "SD"
                      )

points.bf.df <- points.bf.df[,c(5, 1)]
points.bf.df[,2] <- 1 / points.bf.df[,2]

points.bf.df$bf.text[points.bf.df$bf > 1000] <- "BF > 1000"
points.bf.df$bf.text[points.bf.df$bf < 1] <- "BF < 1"
points.bf.df$bf.text[points.bf.df$bf <= 1000 & points.bf.df$bf > 1] <- paste("BF = ", round(points.bf.df$bf[points.bf.df$bf <= 1000 & points.bf.df$bf > 1], 0))


# Version 2: Subsetted data Only Environments 1-3 where people actually peek

data.temp <- subset(participants, environment %in% 1:3 & is.finite(g1.final.points) & 
                      (game.type %in% "k" | g1.n.peeks >= 1))


points.frequentist2 <- lm(g1.final.points ~ game.type.f * environment.f,
                    data = data.temp
                     )

points.ao2.bf <- anovaBF(g1.final.points ~ game.type.f + environment.f + game.type.f*environment.f,
                     data = data.temp, whichModels = "top"
                     )

points.lm2.bf <- lmBF(g1.final.points ~ game.type.f + environment.f + game.type.f*environment.f,
                     data = data.temp
                     )

points.lm2.bf.post <- posterior(points.lm2.bf, iterations = 1e5)


points.bf2.df <- extractBF(points.ao2.bf)
row.names(points.bf2.df) <- NULL
points.bf2.df$Factor <- c("Environment X Game Type",
                      "Environment",
                      "Game Type"
                      )

points.bf2.df <- points.bf2.df[,c(5, 1)]
points.bf2.df[,2] <- 1 / points.bf2.df[,2]

points.bf2.df$bf.text[points.bf2.df$bf > 1000] <- "BF > 1000"
points.bf2.df$bf.text[points.bf2.df$bf < 1] <- "BF < 1"
points.bf2.df$bf.text[points.bf2.df$bf <= 1000 & points.bf2.df$bf > 1] <- paste("BF = ", round(points.bf2.df$bf[points.bf2.df$bf <= 1000 & points.bf2.df$bf > 1], 0))

chains <- as.data.frame(points.lm2.bf.post)

e1ve2.hdi <- hdi.text.fun(unlist(chains["environment.f-1"] - chains["environment.f-2"]),
                            calculate = F, wordy = T)

e1ve3.hdi <- hdi.text.fun(unlist(chains["environment.f-1"] - chains["environment.f-3"]),
                            calculate = F, wordy = T)

e2ve3.hdi <- hdi.text.fun(unlist(chains["environment.f-2"] - chains["environment.f-3"]),
                            calculate = F, wordy = T)

ptk.v.k.hdi <- hdi.text.fun(unlist(chains["game.type.f-ptk"] - chains["game.type.f-k"]),
                            calculate = F, wordy = T)

pak.v.k.hdi <- hdi.text.fun(unlist(chains["game.type.f-pak"] - chains["game.type.f-k"]),
                            calculate = F, wordy = T)

ptk.v.k.pak <- hdi.text.fun(unlist(chains["game.type.f-ptk"] - chains["game.type.f-pak"]),
                            calculate = F, wordy = T)

a <- mean(subset(participants, game.type %in% c("ptk", "pak") & is.finite(g1.n.peeks))$g1.n.peeks > 0, na.rm = T)

@

We entered both decision environment and game type as fixed factors. \footnote{Because performance was so poor across all game types in environment 4, we elected to withdraw these data from the ANOVA. Additionally, in the PAK and PTK conditions, we only included participants who peeked at least once (this was the case for \Sexpr{100 * round(a, 2)}\% of participants).

When we include these data, the effect of game type does not appear (BF = \Sexpr{round(points.bf.df$bf[5], 2)})}. We found credible effects for the option environment (\Sexpr{subset(points.bf2.df, Factor == "Environment")$bf.text}, E1--E2 = \Sexpr{e1ve2.hdi}, E1--E3 = \Sexpr{e1ve3.hdi}, E2--E3 = \Sexpr{e1ve3.hdi}). Critically, we found a credible effect of game.type (\Sexpr{subset(points.bf2.df, Factor == "Game Type")$bf.text}: players playing PTK earned more points than those playing K (mean difference = \Sexpr{ptk.v.k.hdi}) and those playing PAK (mean difference = \Sexpr{ptk.v.k.pak}). However, there was no credible difference between PAK and K conditions (mean difference = \Sexpr{pak.v.k.hdi}). While the raw data appeared to reflect an interaction between environment and game type on points earned, the Bayes Factor for the interaction was \Sexpr{round(points.bf2.df$bf[points.bf2.df$Factor == "Environment X Game Type"], 2)} indicating slight support for the null hypothesis.

\subsection{Peeking Rates}

Next, we focused on the two peeking conditions. We sought to answer three questions: First, how do the overall rates of peeking differ between PAK and PTK and how does peeking change over time? Second, how do environmental factors affect peeking rates? Third, in the PAK condition, how does peeking change over time? Finally, What is the relationship between option switching and peeking? 

\subsubsection{Participant-level}

To see how the overall rates of peeking differed between peeking conditions, we calculated the average rate of peeking across participants at each game trial. Results are in Figure~\ref{fig:peekstime} and average peeking rates across trials are presented in Table~\ref{table:peekdf}.

<<echo = F>>=

peeks.frequentist <- lm(g1.n.peeks ~ game.type + n.options.f + option.sd.f,
                     data = subset(participants, game.type %in% c("pak", "ptk")))

peeks.ao.bf <- anovaBF(g1.n.peeks ~ game.type.f + n.options.f + option.sd.f,
                     data = subset(participants, game.type %in% c("pak", "ptk") & is.finite(g1.n.peeks)), 
                     whichModels = "top")
                     
peeks.lm.bf <- lmBF(g1.n.peeks ~ game.type.f + n.options.f + option.sd.f,
                     data = subset(participants, game.type %in% c("pak", "ptk") & is.finite(g1.n.peeks)))

peeks.lm.bf.post <- posterior(peeks.lm.bf, iterations = 1e5)


peeks.bf.df <- extractBF(peeks.ao.bf)
row.names(peeks.bf.df) <- NULL
peeks.bf.df$Factor <- c("Game Type x Options X SD",
                      "Game Type x SD",
                      "SD x Options",
                      "Options x Game Type",
                      "Game Type",
                      "SD",
                      "Options"
                      )

peeks.bf.df <- peeks.bf.df[,c(5, 1)]
peeks.bf.df[,2] <- 1 / peeks.bf.df[,2]

peeks.bf.df$bf.text[peeks.bf.df$bf > 1000] <- "BF > 1000"
peeks.bf.df$bf.text[peeks.bf.df$bf < 1] <- "BF < 1"
peeks.bf.df$bf.text[peeks.bf.df$bf <= 1000] <- paste("BF =", round(peeks.bf.df$bf[peeks.bf.df$bf <= 1000], 0))

pak.v.ptk.hdi <- hdi.text.fun(unlist(as.data.frame(peeks.lm.bf.post)["game.type.f-pak"] - as.data.frame(peeks.lm.bf.post)["game.type.f-ptk"]),
                            calculate = F, wordy = T)

@


<<echo = F, results='asis'>>=
my.fun <- function(x) {return(
  
  paste(round(mean(x), 2), " [", round(betabinom.hdi(x)$lb, 2), ", ", round(betabinom.hdi(x)$ub, 2), "]", sep = "")
  
  
  
)}

peek.rate.df <- subset(actions, game.type != "k") %>% 
  group_by(game.type, n.options, option.sd) %>%
  summarise(
    text = my.fun(mode == "peek")
  )

names(peek.rate.df) <- c("Game Type", "N Options", "Option SD", "Mean Peek Rate")

peek.rate.df$Environment <- rep(1:4, times = 2)

peek.rate.df <- peek.rate.df[,c(1, 5, 2, 3, 4)]

peek.rate.x <- xtable(peek.rate.df, 
                               caption = "Sample mean and 95\\% Highest Density Intervals of peeking rates across environmental conditions", 
                               label = "table:peekdf")

print(peek.rate.x, include.rownames = F,
      sanitize.text.function = function(x){x},  
      floating.environment = getOption("xtable.floating.environment", "table*"))


@


\begin{itemize}

  \item We found a credible effect of game type on number of peeks (\Sexpr{peeks.bf.df$bf.text[peeks.bf.df$Factor == "Game Type"]}: players playing PTK peeked fewer times than those playing PAK (mean difference = \Sexpr{pak.v.ptk.hdi}).

  \item In PAK, peeking rates are highest in the first 10 trials. After the 10th trial, peeking rates do not change substantially over time.
  
  \item In PTK, peeking rates start much higher than in PAK. In the first trial block of PTK, the mean peeking rate was \Sexpr{betabinom.hdi(subset(actions, game.type == "ptk" & trial10.cut == "(0,10]")$mode == "peek")$text}. In contrast in PAK the mean peeking rate was credibly lower at \Sexpr{betabinom.hdi(subset(actions, game.type == "pak" & trial10.cut == "(0,10]")$mode == "peek")$text}. However, in contrast to PAK, peeking rates decreased dramatically in PTK over time.


\end{itemize}

% create peekrates.pdf
<<echo = F, message = F, results = 'hide'>>=
# --------------------------------
# Peek Rate by condition
# peeks over time.pdf
# --------------------------------

pdf("figures/peekrates.pdf", width = 8, height = 4)

par(mfrow = c(1, 2))

for(peek.cond.i in c("pak", "ptk")) {  
  
  par(mar = c(5, 5, 4, 2))
    
    data <- actions
    data$trial.cut <- cut(data$trial, seq(0, 100, 10))
    
    plot(1, xlim = c(1, 10), ylim = c(0, .6), type = "n", 
         xlab = "",
         main = "", xaxt = "n", cex.lab = 1.5, yaxt = "n", ylab = "")
    
    
    if(peek.cond.i == "pak") {mtext("Peeks AND Keeps", 3, line = 1, cex = 1.5)}
    if(peek.cond.i == "ptk") {mtext("Peeks THEN Keeps", 3, line = 1, cex = 1.5)}
    
    mtext("Trial Block", 1, line = 2.7, cex = 1.5)
    mtext("10 Trials per Block", 1, line = 3.7, cex = .8)

    mtext("p(Peek)", 2, line = 3.5, cex 
          = 1.5)
    
    axis(2, at = seq(0, 1, .1), 
         labels = paste(seq(0, 100, 10), "%", sep = ""), 
         las = 1)
    
    #  mtext("Bars are 95\\% HDIs", side = 3)
    
    col.vec <- piratepal("google")
    
    rect(-100, -100, 100, 100, col = gray(.99))
    
    abline(h = seq(0, .6, .2), col = gray(.9), lwd = 2)
    abline(h = seq(.1, .7, .2), col = gray(.9), lwd = 1)
    abline(v = 1:10, col = gray(.9), lwd = 1)
    
    legend("topright", 
           paste("Env", 1:4), 
           lty = 1, col = col.vec, lwd = 3, bg = "white", cex = .6)
    
    rect.min <- .4
    rect.max <- .6
    locations <- c(4, 5, 6, 7)
    rect.width <- .7
    
    
    rect(3.5, .38, 7.5, .55, col = gray(level = 1, alpha = .7), border = "black")

      for(i in 1:4) {
        
        if(peek.cond.i == "pak") {adj <- 8}  
        if(peek.cond.i == "ptk") {adj <- 0}  
        
        binom.hdi <- actions %>%
          filter(conditionNr.c == adj + i) %>%
          mutate(
            trial.cut = cut(trial, breaks = seq(0, 100, 10)),
            peek = mode == "peek"
          ) %>%
          group_by(trial.cut) %>%
          summarise(
            lb = betabinom.hdi(vec = peek)$lb,
            ub = betabinom.hdi(vec = peek)$ub
          )
        
        
        agg <- aggregate(mode == "peek" ~ trial.cut, data = subset(data, conditionNr.c == adj + i), FUN = mean)
        
        lines(agg[,1], agg[,2], col = col.vec[i], lwd = 3)
        
        polygon(c(1:10, 10:1), c(binom.hdi$lb, rev(binom.hdi$ub)), 
                col = transparent(col.vec[i], trans.val = .9), border = NA)
        
        overall <- betabinom.hdi(subset(actions, conditionNr.c == adj + i)$mode == "peek")
        
        # Add overall bin
        
        rect(locations[i] - rect.width / 2, 
             rect.min, 
             locations[i] + rect.width / 2,
             rect.min + overall$mode * (rect.max - rect.min),
             col = col.vec[i])
        
        text(locations[i], rect.min + overall$mode * (rect.max - rect.min), paste(round(overall$mode, 2) * 100, "%", sep = ""), pos = 3, cex = .6)
        # segments(locations[i] + .5, rect.min + overall$lb * (rect.max - rect.min), 
        #          locations[i] + .5, rect.min + overall$ub * (rect.max - rect.min), col = col.vec[i], lwd = 2)
        
        
      }
  
    axis(side = 1, labels = 1:10, at = 1:10, cex.axis = .9)
    text(5.5, .52, "All Blocks", cex = .8)
    
  }
  
  dev.off()

@

\begin{figure}
\includegraphics[width=\columnwidth]{figures/peekrates.pdf}
\caption{Mean peeking rates by trial block (groups of 10 trials) separated by environmental condition. The plot on the left is for PAK (Peeks and Keeps). The plot on the right is for PTK (Peeks then Keeps).}
\label{fig:peekstime}
\end{figure}


%\section{Peek Rates and Earnings}
<<echo = F, eval = F>>=

par(mfrow = c(1, 2))

col.vec <- piratepal("google", length.out = 4, trans = .1)

for(game.type.i in c("pak", "ptk")) {
  
  plot(1, xlim = c(1, 100), ylim = c(-500, 500), type = "n", main = game.type.i, 
       bty = "n", yaxt = "n", ylab = "Final Points")
  axis(2, at = seq(-500, 500, 100), las = 1)
  abline(h = seq(-500, 500, 100), lwd = c(.5, 1), col = gray(.5))
  
  for(env.i in 1:4) {
    
     data <- subset(participants, game.type == game.type.i & environment == env.i & is.finite(g1.final.points) & is.finite(g1.n.peeks))
     with(data, points(g1.n.peeks, g1.final.points, pch = 16, col = col.vec[env.i]))
     
     
     # Add simulation curve
     

       
       sim.agg <- subset(points.agg, n.bad == condition.table$n.options[condition.table$environment == env.i] & 
                           sd == condition.table$option.sd[condition.table$environment == env.i])
       
       lines(sim.agg$n.peeks, sim.agg$total.points.mean, col = col.vec[env.i], lwd = 3)
       
     }
     
     
     
     # Fit quadratic regression line
     
#      quadloss.fun <- function(par) {
#        
#        a <- par[1]
#        b <- par[2]
#        c <- par[3]
#        sigma <- par[4]
#        
#        if(sigma > 0) {
#        
#        lik <- dnorm(x = data$g1.final.points,
#                     mean = a * data$g1.n.peeks ^ 2 + b * data$g1.n.peeks + c,
#                     sd = sigma
#                     )
#        
#        loglik <- log(lik)
#        dev <- -2 * sum(loglik)
#        
#        }
#        
#        if(sigma <= 0) {dev <- 99999999}
#        
#        
# 
#        
#        return(dev)
#        
#      }
#        
#      result <- optim(c(0, 0, 0, 100), fn = quadloss.fun, hessian = T)  
#      final.par <- result$par
#      
#      
#      curve(final.par[1] * x^2 + final.par[2] * x + final.par[3], add = T, col = col.vec[env.i])
#        
     
     
  }
  

@

\subsection{Trial-level}

% p(Peek | Many variables)
<<echo = F, message = F, results = 'hide'>>=
actions$peek.log <- actions$mode == "peek"
actions$last.outcome.pos.log <- actions$option.prior.outcome > 0
actions$option.prior.mean.pos.log <- actions$option.prior.mean > 0

whenpeek.mod <- MCMCglmm(peek.log ~ trials.since.last.samp + 
                            last.outcome.pos.log + 
                            option.prior.mean.pos.log + trial, 
                 family = "categorical",
                 data = subset(actions, 
                               game.type == "pak" & 
                                 is.finite(trials.since.last.samp) &
                                 is.finite(last.outcome.pos.log) &
                                 is.finite(option.prior.mean.pos.log)),
                 nitt = 1e4
                   )
@

In order to determine which factors were credibly correlated with the probability of peeking for participants in the Peeks and Keeps condition, we conducted a Bayesian Logistic regression analysis at the trial level. The dependent variable was a binary coded variable with a 1 indicating that the player had peeked that trial, and a 0 indicating that the player had keeped. We entered four fixed independent variables in the regression. The first two variables were time based: trial, the current trial number, and temporal distance - the number of trials since the player had last sampled from the option. The second two independent variables were a function of prior outcomes from the sampled option: prior mean positive - a binary variable with a 1 indicating that the option's prior sample mean was positive and 0 otherwise, and prior outcome positive - a binary variable with a 1 indicating that the option's prior outcome was positive.

\begin{itemize}

% p(Peek | Last outcome valence)
<<echo = F, message = F, results = 'hide'>>=

# What is the probability of peeking given the last outcome valence from that option?

p.peek.g.pos <- with(subset(actions, game.type == "pak" & option.prior.outcome > 0), mean(mode == "peek", na.rm = T))

p.peek.g.neg <- with(subset(actions, game.type == "pak" & option.prior.outcome < 0), mean(mode == "peek", na.rm = T))

p.peek.mod <- compare.Coins(
  
  Heads.A = with(subset(actions, game.type == "pak" & option.prior.outcome > 0), sum(mode == "peek", na.rm = T)),
  Flips.A = with(subset(actions, game.type == "pak" & option.prior.outcome > 0), sum(mode %in% c("peek", "keep"), na.rm = T)),
   Heads.B = with(subset(actions, game.type == "pak" & option.prior.outcome < 0), sum(mode == "peek", na.rm = T)),
  Flips.B = with(subset(actions, game.type == "pak" & option.prior.outcome < 0), sum(mode %in% c("peek", "keep"), na.rm = T))
)
  
@

% p(Peek | Temporal Distance plot)

<<echo = F, results = "hide">>=

pdf("figures/peekdistance.pdf", width = 8, height = 5)


get.binom <- function(x, output) {
  
  if(output == "lb") {  return(betabinom.hdi(x, alpha.prior = 1, beta.prior = 1, na.rm = T)$lb)}
  if(output == "ub") {  return(betabinom.hdi(x, alpha.prior = 1, beta.prior = 1, na.rm = T)$ub)}
  
}

peek.distance.df <- actions %>%
  subset(game.type == "pak") %>%
  group_by(trials.since.last.samp) %>%
  summarise(
    freq = n(),
    p.peek = mean(mode == "peek", na.rm = T),
    lb = get.binom(mode == "peek", output = "lb"),
    ub = get.binom(mode == "peek", output = "ub")
  )

plot(1, xlim = c(1, 15), ylim = c(0, .6), 
     type = "n", xaxt = "n", xlab = "Temporal Distance",
     ylab = "p(Peek)")
axis(side = 1, at = 1:15)

rect(-1000, -1000, 1000, 1000, col = gray(.96))

abline(h = seq(0, 1, .1), lwd = c(1, 2), col = gray(1))
abline(v = 1:15, col = gray(1), lwd = c(1, 2))

abline(h = .5)


with(peek.distance.df,
     points(trials.since.last.samp, 
            p.peek, 
            cex = (freq) ^ .2,
            pch = 21,
            bg = "white"
            )
     )

with(peek.distance.df, segments(trials.since.last.samp, lb, trials.since.last.samp, ub))

dev.off()

@

\begin{figure}
\includegraphics[width=\columnwidth]{figures/peekdistance.pdf}
\caption{Mean peeking rates by temporal distance - the number of trials since the selected option was last observed by the participant - across all participants in the Peeks and Keeps conditions. The size of the symbols are an increasing function of the relative frequency of observations in the dataset. The segments indicate 95\% HDIs.}
\label{fig:peeksdistance}
\end{figure}



\item We begin with the time based variables. There were credible partial effect of both trial and temporal distance on peek rates. Players were credibly less likely to peek on later trials than earlier trials (\Sexpr{hdi.text.fun(whenpeek.mod$Sol[,5], calculate = F, wordy = T)}), and were credibly more likely to peek as temporal distance increased (\Sexpr{hdi.text.fun(whenpeek.mod$Sol[,2], calculate = F, wordy = T)}) (See Figure \ref{fig:peeksdistance}. In other words, the longer it had been since a player had sampled an option, the more likely it is that he/she peeked. For example, when temporal distance was 2, the mean peek rate was \Sexpr{round(mean(subset(actions, game.type == "pak" & trials.since.last.samp == 2)$mode == "peek", na.rm = T), 2) * 100}\%, compared to a mean peek rate of \Sexpr{round(mean(subset(actions, game.type == "pak" & trials.since.last.samp == 10)$mode == "peek", na.rm = T), 2) * 100}\% when the temporal distance was 10.


 \item Now we move to the outcome based variables. Participants were credibly more likely to peek at an option when the last outcome observed from that option was negative (\Sexpr{round(p.peek.g.neg, 2) * 100}\%, 95\% HDI: [\Sexpr{round(p.peek.mod$Summary$quantiles[3,1], 2) * 100}, \Sexpr{round(p.peek.mod$Summary$quantiles[3,5], 2) * 100}\%]) than when it was positive (\Sexpr{round(p.peek.g.pos, 2) * 100}\%, 95\% HDI: [\Sexpr{round(p.peek.mod$Summary$quantiles[2,1], 2)*100}\%, \Sexpr{round(p.peek.mod$Summary$quantiles[2,5], 2) * 100}\%], 95\% HDI of difference: [\Sexpr{round(p.peek.mod$Summary$quantiles[4,1], 2)*100}\%, \Sexpr{round(p.peek.mod$Summary$quantiles[4,5], 2)*100}\%]).



\end{itemize}



\section{Strategy Fitting}


See pak_strategyfitting.R

% Individual participant curves
% pak participant curves.pdf
<<echo = F, eval = F>>=
# --------------------------------
# Plot individual participant's data
# pak participant curves.pdf
# --------------------------------
{


pdf("figures/pak participant curves2.pdf", width = 12, height = 8)
  
worker.vec <- unique(subset(participants, game.type %in% c("pak", "ptk"))$workerid)

for(i in worker.vec) {
  
  actions.i <- subset(actions, workerid == i)
  
  n.games <- length(unique(actions.i$playergamenum))
  
  n.options <- actions.i$n.options[1]
      worker.name <- i
    game.type <- actions.i$game.type[1]
    option.sd <- actions.i$option.sd[1]
    
    
  selection.ls <- vector("list", n.games)
  outcome.ls <- vector("list", n.games)
  action.ls <- vector("list", n.games)
  
  best.option.vec <- c()
  
  for(j in 1:n.games) {
    
      selection.ls[[j]] <- paste(subset(actions.i, playergamenum == j)$selection)
      outcome.ls[[j]] <- subset(actions.i, playergamenum == j)$outcome
      action.ls[[j]] <- paste(subset(actions.i, playergamenum == j)$mode)
      
      
      option.order.i <- subset(optionorder, workerid == i & playergamenum == j)$optionorder
      option.order.i <- as.numeric(unlist(str_split(option.order.i, ",")))
      best.option.i <- which(option.order.i == 1)
      best.option.vec <- c(best.option.vec, best.option.i)
  
  }
      
  if(mean(unlist(lapply(1:n.games, function(x) {length(selection.ls[[x]]) == 100}))) == 1) {
  
  plot.pak(
           n.options = n.options,
           worker.name = i,
           game.type = game.type,
           option.sd = option.sd,
           selection.ls = selection.ls,
           outcome.ls = outcome.ls,
           action.ls = action.ls,
           best.option.vec = LETTERS[best.option.vec],
           col.vec = piratepal(palette = "google", trans = .3))
  
  }
  
}

dev.off()
}
@

% Participant peeks over time
% participant peeks over time.pdf
<<echo = F, eval = F>>=

{
  
pdf("figures/participant peeks over time.pdf", width = 16, height = 12)

cond.vec <- c(1:4, 9:12)

col.1 <- gray(.9)
col.2 <- "white"

line.col.vec <- rep(piratepal("info")[c(1, 2, 4, 5)], 4)

    my.text <- "Peek Rate"
    cond.vec <- cond.vec <- c(1:4, 9:12)
    layout(matrix(c(1:5, 1, 6:9, 1, 10:13, 1, 14:17), nrow = 5, ncol = 4, byrow = F), 
           widths = rep(5, 4), heights = c(2, rep(5, 4)))

  

# Header plot
  
  par(mar = rep(0, 4))
  
  plot(1, type = "n", xaxt = "n", xlab = "", xlim = c(0, 1), ylim = c(0, 1), bty = "n")
  
  text(.5, .5, my.text,  cex = 3)

  
for(cond.i in cond.vec) {

game.type.i <- condition.table$game.type[condition.table$uni.condition == cond.i]  
n.options.i <- condition.table$n.options[condition.table$uni.condition == cond.i]
option.sd.i <- condition.table$option.sd[condition.table$uni.condition == cond.i]

data <- subset(actions, conditionNr.c == cond.i)
data$peek.bin <- data$mode == "peek"

worker.vec <- unique(data$workerid)


# Top plot
par(mar = c(3, 4, 4, 1) + .1)
plot(1, xlim = c(0, 100), ylim = c(0, 1), type = "n", bty = "n", 
     xlab = "", ylab = "p(Peek)", 
     main = paste("Game = ", game.type.i, "\n",
                  "N = ", n.options.i, "\n",
                  "SD = ", option.sd.i, sep = ""
                  ), 
     cex.main = 1)

abline(h = seq(0, 1, .1), lwd = c(1, 2), col = gray(.9))
abline(v = seq(0, 100, 10), lwd = c(1, 2), col = gray(.9))



avg <- data %>%
  group_by(trial) %>%
  summarise(
    peek.mean = mean(peek.bin)
  )

lines(1:100, avg$peek.mean, col = line.col.vec[cond.i], lwd = 5)

mtext(paste("Mean", round(mean(data$peek.bin), 2), sep = "\n"), side = 3, at = 90, line = 1)

# Bottom Plot

par(mar = c(3, 4, 0, 1) + .1)
plot(1, xlim = c(0, 100), ylim = c(0, length(worker.vec) + 2), 
     type = "n", ylab = "", yaxt = "n", bty = "n")

for(i in 1:length(worker.vec)) {
 
vec <- data$peek.bin[data$workerid == worker.vec[i]]
   
 col.vec <- rep(col.1, 100)
 col.vec[vec == FALSE] <- col.2
 
 rect(1:100 - .5, 
      rep(i, 100) - .5, 
      1:100 + .5, 
      rep(i, 100) + .5,
      col = col.vec, border = "white", lwd = .5)
 
}

mtext(text = 1:length(worker.vec), at = 1:length(worker.vec), side = 2, cex = .4, las = 1)

}


dev.off()
}
@

% Participant option switch over time
% participant option switch over time.pdf
<<echo = F, eval = F>>=

{
pdf("figures/participant option switch over time.pdf", width = 24, height = 12)


col.1 <- gray(.5)
col.2 <- "white"
line.col.vec <- rep(piratepal("info")[c(1, 2, 4, 5)], 4)

my.text <- "Option Switch Rate"
cond.vec <- cond.vec <- c(1:4, 9:12, 5:8)

layout(matrix(c(1:5, 1, 6:9, 1, 10:13, 1, 14:17, 1, 18:21, 1, 22:25), nrow = 5, ncol = 6, byrow = F), 
       widths = rep(5, 6), heights = c(1.5, rep(5, 4)))

# Header plot

par(mar = rep(0, 4))

plot(1, type = "n", xaxt = "n", xlab = "", xlim = c(0, 1), ylim = c(0, 1), bty = "n")

text(.5, .5, my.text,  cex = 3)

for(cond.i in cond.vec) {
  
  game.type.i <- condition.table$game.type[condition.table$uni.condition == cond.i]  
  n.options.i <- condition.table$n.options[condition.table$uni.condition == cond.i]
  option.sd.i <- condition.table$option.sd[condition.table$uni.condition == cond.i]
  
  data <- subset(actions, conditionNr.c == cond.i)
  data$peek.bin <- data$mode == "peek"
  
  worker.vec <- unique(data$workerid)
  
  
  # Top plot
  par(mar = c(2, 4, 4, 1) + .1)
  plot(1, xlim = c(0, 100), ylim = c(0, 1), type = "n", bty = "n", 
       xlab = "", ylab = "p(Peek)", 
       main = paste("Game = ", game.type.i, "\n",
                    "N = ", n.options.i, "\n",
                    "SD = ", option.sd.i, sep = ""
       ), 
       cex.main = 1)
  
  mtext(paste("Mean", round(mean(data$option.switch, na.rm = T), 2), sep = "\n"), side = 3, at = 90, line = 1)
  
  
  abline(h = seq(0, 1, .1), lwd = c(1, 2), col = gray(.9))
  abline(v = seq(0, 100, 10), lwd = c(1, 2), col = gray(.9))
  

    
    avg <- data %>%
      group_by(trial) %>%
      summarise(
        switch.mean = mean(option.switch)
      )
    
    lines(1:100, avg$switch.mean, col = line.col.vec[cond.i], lwd = 5)
    

  
  # Bottom Plot
  
  par(mar = c(3, 4, 0, 1) + .1)
  plot(1, xlim = c(0, 100), ylim = c(0, length(worker.vec) + 2), 
       type = "n", ylab = "Participants", yaxt = "n", bty = "n")
  
  for(i in 1:length(worker.vec)) {
    

   vec <- data$option.switch[data$workerid == worker.vec[i]]
    
    col.vec <- rep(col.1, 100)
    col.vec[vec == FALSE] <- col.2
    
    rect(1:100 - .5, 
         rep(i, 100) - .5, 
         1:100 + .5, 
         rep(i, 100) + .5,
         col = col.vec, border = "white", lwd = .5)
    
  }
  
  mtext(text = 1:length(worker.vec), at = 1:length(worker.vec), side = 2, cex = .4, las = 1)
  
}


dev.off()
}

@


<<eval = F, echo = F>>=

actions <- actions %>% mutate(
  
  last.outcome.pos = option.prior.outcome > 0,
  last.mean.pos = option.prior.mean > 0
  
)


plot(1, xlim = c(0, 3), ylim = c(0, 1), 
     type = "n", xaxt = "n", ylab = "p(Peek)",
     xlab = "Option Switch", main = "p(Peek) | Option Switch")

rect(-100, -100, 100, 100, col = gray(.96))
abline(h = seq(0, 1, .1), lwd = c(1, 2), col = "white")

add.binom.p.bar(subset(actions, game.type == "pak" & option.switch == F)$mode == "peek", location = 1, bar.col = piratepal("google", length.out = 5)[1], bar.label = "Switch Option")


add.binom.p.bar(subset(actions, game.type == "pak" & option.switch == T)$mode == "peek", location = 2, bar.col = piratepal("google", length.out = 5)[2], bar.label = "Same Option")




plot(1, xlim = c(0, 3), ylim = c(0, 1), 
     type = "n", xaxt = "n", ylab = "p(Peek)",
     xlab = "Option Switch", main = "p(Peek) | Last outcome valence")

rect(-100, -100, 100, 100, col = gray(.96))
abline(h = seq(0, 1, .1), lwd = c(1, 2), col = "white")

add.binom.p.bar(subset(actions, game.type == "pak" & last.outcome.pos == T)$mode == "peek", location = 1, bar.col = piratepal("google", length.out = 5)[1], bar.label = "Positive")


add.binom.p.bar(subset(actions, game.type == "pak" & last.outcome.pos == F)$mode == "peek", location = 2, bar.col = piratepal("google", length.out = 5)[2], bar.label = "Neg")


peek.by.trialssince <- actions %>% 
  filter(game.type == "pak") %>%
  group_by(trials.since.last.samp) %>%
  summarise(
    
    peek.rate = mean(mode == "peek", na.rm = T),
    n = n()
    
  )




with(subset(peek.by.trialssince, trials.since.last.samp <= 30), 
     plot(trials.since.last.samp, peek.rate, cex = log(n, base = 8), 
          ylim = c(0, .7), pch = 21, 
          bg = piratepal("info", trans = .5, length.out = 1),
          xlab = "Trials since last sample",
          ylab = "p(Peek)",
          main = "p(Peek) | Time since last sample"
          
          ))

abline(h = .5, lty = 2)









actions <- actions %>% mutate(
  
  prior.mean.pos = option.prior.mean > 0,
  prior.outcome.pos = option.prior.outcome > 0
  
)

actions %>% filter(game.type == "pak") %>% group_by(prior.outcome.pos) %>%
  summarise(
    change.option = mean(option.switch, na.rm = T),
    peek = mean(mode == "peek", na.rm = T),
    n = n()
    
  )

actions %>% filter(game.type == "pak") %>% group_by(prior.mean.pos) %>%
  summarise(
    change.option = mean(option.switch, na.rm = T),
    peek = mean(mode == "peek", na.rm = T),
    n = n()
    
  )



@


\section{Study 2 (Proposed)}

\subsection{Design}


Changes from first study

\begin{itemize}

  \item Increase number of participants per cell (100 each = 600 participants).
  \item Participants are given complete information about environments prior to game. Then we can argue for 'optimal' behavior.
  \item Players play 3 games. Points are summed over all games and converted to a bonus. This should motivate people to maximize earnings -- and minimize losses -- in each game.
  \item Get rid of really easy (n = 2, sd = 5) and really difficult (n = 4, sd = 20) conditions.
  

\end{itemize}

Between-Subjects (9 conditions)

Game (3)

\begin{itemize}

  \item Game Type (Keeps, Peeks and Keeps, Peeks Then Keeps)

\end{itemize}

Environments (3)

\begin{itemize}

  \item N.Options (2, 3, 4)
  \item Option SD (10)

\end{itemize}

Within-Subjects (3)

N-Games (3)





\section{Discussion}




\section{Conclusion}


\section{Appendix}


\section{Additional Figures}



\bibliography{/Users/Nathaniel/Dropbox/Nathaniel_BibTek}
\end{document}